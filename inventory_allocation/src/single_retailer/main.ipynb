{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://towardsdatascience.com/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from create_historical_demand import create_demand_history\n",
    "from agent import Agent\n",
    "from environment import InvOptEnv\n",
    "\n",
    "from ss_profit_calc import profit_calculation_s_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=7, action_size=21, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, n_episodes=1000, max_t=10000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "  '''Deep Q-Learning\n",
    "\n",
    "  Params\n",
    "  ======\n",
    "    n_episodes (int): maximum number of training epsiodes\n",
    "    max_t (int): maximum number of timesteps per episode\n",
    "    eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "    eps_end (float): minimum value of epsilon\n",
    "    eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
    "\n",
    "  '''\n",
    "  scores = [] # list containing score from each episode\n",
    "  eps = eps_start\n",
    "  for i_episode in range(1, n_episodes+1):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for t in range(max_t):\n",
    "      action = agent.act(state,eps)\n",
    "      next_state,reward,done = env.step(action)\n",
    "      agent.step(state,action,reward,next_state,done)\n",
    "      ## above step decides whether we will train(learn) the network\n",
    "      ## actor (local_qnetwork) or we will fill the replay buffer\n",
    "      ## if len replay buffer is equal to the batch size then we will\n",
    "      ## train the network or otherwise we will add experience tuple in our\n",
    "      ## replay buffer.\n",
    "      state = next_state\n",
    "      score += reward\n",
    "      if done:\n",
    "        print('episode '+str(i_episode).zfill(len(str(n_episodes)))+':', score)\n",
    "        scores.append(score)\n",
    "        break\n",
    "    eps = max(eps*eps_decay, eps_end)## decrease the epsilon\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_history = create_demand_history()\n",
    "plt.hist(demand_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InvOptEnv(demand_history)\n",
    "scores= dqn(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.qnetwork_local.state_dict(), './agent_qnetwork_local_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_s_list = []\n",
    "for S in range(1,61): # give a little room to allow S to exceed the capacity\n",
    "  for s in range(0,S):\n",
    "    s_s_list.append([s,S])\n",
    "\n",
    "profit_sS_list = []\n",
    "for sS in s_s_list:\n",
    "  profit_sS_list.append(profit_calculation_s_s(sS[0],sS[1],demand_history))\n",
    "\n",
    "best_sS_profit = np.max(profit_sS_list)\n",
    "best_sS = s_s_list[np.argmax(profit_sS_list)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "any-project-template-pwAWjWQM-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
